{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b2bb01c",
   "metadata": {},
   "source": [
    "# OPS-SAT case starter-kit notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9611e0",
   "metadata": {},
   "source": [
    "ESA's [Kelvins](https://kelvins.esa.int) competition \"[the OPS-SAT case](https://kelvins.esa.int/opssat/home/)\" is a novel data-centric challenge that asks you to work with the raw data of a satellite and very few provided labels to find the best parameters for a given machine learning model. Compared to previous competitions on Kelvins (like the [Pose Estimation](https://kelvins.esa.int/pose-estimation-2021/) or the [Proba-V Super-resolution challenge](https://kelvins.esa.int/proba-v-super-resolution/)) where the test-set is provided and the infered results are submitted, for the OPS-SAT case, we will run inference on the Kelvins server directly! This notebooks contains examples on how you can load your data and train an **EfficientNetLite0** model by only using the 80-labeled images provided. Therefore, the directory `images`, containing unlabeld patches and included in the training dataset is not used for this notebook. However, competitors are encouraged to use these patches to improve the model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2993066d",
   "metadata": {},
   "source": [
    "# 1. Module imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039fe190",
   "metadata": {},
   "source": [
    "If you do not have a GPU, uncomment and run the next commands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d7b9414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "file = '/home/phillip/PycharmProjects/the_opssat_case_starter_kit'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354dcd3",
   "metadata": {},
   "source": [
    "Other imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a045946a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 17:41:20.120848: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-17 17:41:20.120883: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from efficientnet_lite import EfficientNetLiteB0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c369e7",
   "metadata": {},
   "source": [
    "# 2. Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce95a9",
   "metadata": {},
   "source": [
    "You can use this function to load your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b93e7152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_from_path(dataset_path, split: bool = True, _seed: int = 142):\n",
    "    \"\"\" Get images from path and normalize them applying channel-level normalization. \"\"\"\n",
    "\n",
    "    # loading all images in one large batch\n",
    "    if split:\n",
    "        tf_train_data = tf.keras.utils.image_dataset_from_directory(dataset_path, image_size=input_shape[:2], shuffle=False, validation_split=0.2, subset=\"training\", seed=_seed)\n",
    "        tf_eval_data = tf.keras.utils.image_dataset_from_directory(dataset_path, image_size=input_shape[:2], shuffle=False, validation_split=0.2, subset=\"validation\", seed=_seed)\n",
    "    \n",
    "        # extract images and targets\n",
    "        for tf_eval_images, tf_eval_targets in tf_eval_data:\n",
    "            break\n",
    "\n",
    "        for tf_train_images, tf_train_targets in tf_train_data:\n",
    "            break\n",
    "\n",
    "        return tf.convert_to_tensor(tf_train_images), tf_train_targets, tf.convert_to_tensor(tf_eval_images), tf_eval_targets, tf_eval_data\n",
    "    else:\n",
    "        # loading all images in one large batch\n",
    "        tf_eval_data = tf.keras.utils.image_dataset_from_directory(dataset_path, image_size=input_shape[:2], shuffle=False, \n",
    "                                                                   batch_size=100000)\n",
    "\n",
    "        # extract images and targets\n",
    "        for tf_eval_images, tf_eval_targets in tf_eval_data:\n",
    "            break\n",
    "\n",
    "        return tf.convert_to_tensor(tf_eval_images), tf_eval_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e397f",
   "metadata": {},
   "source": [
    "# 3. Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03702ebb",
   "metadata": {},
   "source": [
    "The network architecture used for OPS-SAT is **EfficientNetLite0**. We would like to thank Sebastian for making a Keras implementation of EfficientNetLite publicly available under the Apache 2.0 License: https://github.com/sebastian-sz/efficientnet-lite-keras. Our Version of this code has been modified to better fit our purposes. For example, we removed the ReLU \"stem_activation\" to better match a related efficientnet pytorch implementation. In any way, **you have to use the model architecture that we provide in our [starter-kit](https://gitlab.com/EuropeanSpaceAgency/the_opssat_case_starter_kit).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d85d1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 17:41:22.475094: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-17 17:41:22.475131: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-17 17:41:22.475159: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (laptop): /proc/driver/nvidia/version does not exist\n",
      "2022-10-17 17:41:22.475466: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1a_ same\n",
      "block2a_ ((1, 1), (1, 1))\n",
      "block2a_ valid\n",
      "block2b_ same\n",
      "block3a_ ((2, 2), (2, 2))\n",
      "block3a_ valid\n",
      "block3b_ same\n",
      "block4a_ ((1, 1), (1, 1))\n",
      "block4a_ valid\n",
      "block4b_ same\n",
      "block4c_ same\n",
      "block5a_ same\n",
      "block5b_ same\n",
      "block5c_ same\n",
      "block6a_ ((2, 2), (2, 2))\n",
      "block6a_ valid\n",
      "block6b_ same\n",
      "block6c_ same\n",
      "block6d_ same\n",
      "block7a_ same\n"
     ]
    }
   ],
   "source": [
    "input_shape = (200, 200, 3)   # input_shape is (height, width, number of channels) for images\n",
    "num_classes = 8\n",
    "model = EfficientNetLiteB0(classes=num_classes, weights=None, input_shape=input_shape, classifier_activation=None)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c318836",
   "metadata": {},
   "source": [
    "# 4. Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b233b",
   "metadata": {},
   "source": [
    "Uncomment next line and adjust with the path of your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5a885c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_path=os.path.join(file, 'ops_sat_competiton_official_training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f568413",
   "metadata": {},
   "source": [
    "In this notebook, classical supervised learning is used. Therefore, remember to remove the subdirectory `images` containing unlabeled patches before loading the dataset to perform training correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c99d3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 746 files belonging to 8 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 17:41:24.301058: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 358080000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "#Loading dataset\n",
    "x_train, y_train=get_images_from_path(training_dataset_path, split=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b368a33",
   "metadata": {},
   "source": [
    "# 5. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72364046",
   "metadata": {},
   "source": [
    "We provide now an example on how you can train your model by using standard supervised learning. Training loss (`SparseCategoricalCrossentropy`) and `Accuracy` are shown for simplicity and for an easier interpretation of the training outcome, despite your submission will be evaluated by using the metric **1 - Cohen's kappa** [metric](https://en.wikipedia.org/wiki/Cohen's_kappa). For more information on scoring, please refer to [Scoring](https://kelvins.esa.int/opssat/scoring/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1cb6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc20e92",
   "metadata": {},
   "source": [
    "With this model and the dataset provided, please do your best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f6638e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "94/94 [==============================] - 71s 689ms/step - loss: 3.9114 - sparse_categorical_accuracy: 0.2131\n",
      "Epoch 2/40\n",
      "94/94 [==============================] - 70s 749ms/step - loss: 3.0040 - sparse_categorical_accuracy: 0.2198\n",
      "Epoch 3/40\n",
      "94/94 [==============================] - 72s 766ms/step - loss: 2.6798 - sparse_categorical_accuracy: 0.2708\n",
      "Epoch 4/40\n",
      "94/94 [==============================] - 73s 780ms/step - loss: 2.3865 - sparse_categorical_accuracy: 0.2962\n",
      "Epoch 5/40\n",
      "94/94 [==============================] - 75s 800ms/step - loss: 2.2368 - sparse_categorical_accuracy: 0.3231\n",
      "Epoch 6/40\n",
      "94/94 [==============================] - 75s 797ms/step - loss: 2.1368 - sparse_categorical_accuracy: 0.3164\n",
      "Epoch 7/40\n",
      "94/94 [==============================] - 76s 808ms/step - loss: 1.9568 - sparse_categorical_accuracy: 0.3324\n",
      "Epoch 8/40\n",
      "94/94 [==============================] - 77s 815ms/step - loss: 1.8431 - sparse_categorical_accuracy: 0.3740\n",
      "Epoch 9/40\n",
      "94/94 [==============================] - 78s 830ms/step - loss: 1.6289 - sparse_categorical_accuracy: 0.4169\n",
      "Epoch 10/40\n",
      "94/94 [==============================] - 78s 825ms/step - loss: 1.6272 - sparse_categorical_accuracy: 0.4424\n",
      "Epoch 11/40\n",
      "94/94 [==============================] - 78s 826ms/step - loss: 1.6562 - sparse_categorical_accuracy: 0.4102\n",
      "Epoch 12/40\n",
      "94/94 [==============================] - 78s 834ms/step - loss: 1.5055 - sparse_categorical_accuracy: 0.4598\n",
      "Epoch 13/40\n",
      "94/94 [==============================] - 78s 834ms/step - loss: 1.5009 - sparse_categorical_accuracy: 0.4786\n",
      "Epoch 14/40\n",
      "94/94 [==============================] - 79s 839ms/step - loss: 1.4532 - sparse_categorical_accuracy: 0.4571\n",
      "Epoch 15/40\n",
      "94/94 [==============================] - 78s 833ms/step - loss: 1.4835 - sparse_categorical_accuracy: 0.4812\n",
      "Epoch 16/40\n",
      "94/94 [==============================] - 104s 1s/step - loss: 1.4311 - sparse_categorical_accuracy: 0.4933\n",
      "Epoch 17/40\n",
      "94/94 [==============================] - 104s 1s/step - loss: 1.3886 - sparse_categorical_accuracy: 0.4718\n",
      "Epoch 18/40\n",
      "94/94 [==============================] - 97s 1s/step - loss: 1.3659 - sparse_categorical_accuracy: 0.5161\n",
      "Epoch 19/40\n",
      "94/94 [==============================] - 109s 1s/step - loss: 1.3184 - sparse_categorical_accuracy: 0.4920\n",
      "Epoch 20/40\n",
      "94/94 [==============================] - 97s 1s/step - loss: 1.3455 - sparse_categorical_accuracy: 0.5107\n",
      "Epoch 21/40\n",
      "94/94 [==============================] - 103s 1s/step - loss: 1.2406 - sparse_categorical_accuracy: 0.5268\n",
      "Epoch 22/40\n",
      "94/94 [==============================] - 98s 1s/step - loss: 1.2849 - sparse_categorical_accuracy: 0.5228\n",
      "Epoch 23/40\n",
      "94/94 [==============================] - 102s 1s/step - loss: 1.3082 - sparse_categorical_accuracy: 0.5080\n",
      "Epoch 24/40\n",
      "94/94 [==============================] - 100s 1s/step - loss: 1.2348 - sparse_categorical_accuracy: 0.5509\n",
      "Epoch 25/40\n",
      "94/94 [==============================] - 96s 1s/step - loss: 1.2428 - sparse_categorical_accuracy: 0.5469\n",
      "Epoch 26/40\n",
      "94/94 [==============================] - 101s 1s/step - loss: 1.1408 - sparse_categorical_accuracy: 0.6032\n",
      "Epoch 27/40\n",
      "94/94 [==============================] - 94s 998ms/step - loss: 1.1365 - sparse_categorical_accuracy: 0.5684\n",
      "Epoch 28/40\n",
      "94/94 [==============================] - 94s 999ms/step - loss: 1.1494 - sparse_categorical_accuracy: 0.5818\n",
      "Epoch 29/40\n",
      "94/94 [==============================] - 94s 1s/step - loss: 1.0945 - sparse_categorical_accuracy: 0.5952\n",
      "Epoch 30/40\n",
      "94/94 [==============================] - 79s 837ms/step - loss: 1.1125 - sparse_categorical_accuracy: 0.5724\n",
      "Epoch 31/40\n",
      "94/94 [==============================] - 83s 879ms/step - loss: 1.0336 - sparse_categorical_accuracy: 0.6260\n",
      "Epoch 32/40\n",
      "94/94 [==============================] - 89s 948ms/step - loss: 1.0265 - sparse_categorical_accuracy: 0.6180\n",
      "Epoch 33/40\n",
      "94/94 [==============================] - 107s 1s/step - loss: 1.0630 - sparse_categorical_accuracy: 0.6032\n",
      "Epoch 34/40\n",
      "94/94 [==============================] - 105s 1s/step - loss: 0.9995 - sparse_categorical_accuracy: 0.6394\n",
      "Epoch 35/40\n",
      "94/94 [==============================] - 80s 848ms/step - loss: 0.9808 - sparse_categorical_accuracy: 0.6501\n",
      "Epoch 36/40\n",
      "94/94 [==============================] - 81s 862ms/step - loss: 0.9704 - sparse_categorical_accuracy: 0.6340\n",
      "Epoch 37/40\n",
      "94/94 [==============================] - 80s 854ms/step - loss: 0.8884 - sparse_categorical_accuracy: 0.6863\n",
      "Epoch 38/40\n",
      "94/94 [==============================] - 81s 856ms/step - loss: 0.9113 - sparse_categorical_accuracy: 0.6568\n",
      "Epoch 39/40\n",
      "94/94 [==============================] - 82s 870ms/step - loss: 0.8499 - sparse_categorical_accuracy: 0.6756\n",
      "Epoch 40/40\n",
      "94/94 [==============================] - 81s 860ms/step - loss: 0.8742 - sparse_categorical_accuracy: 0.6863\n"
     ]
    }
   ],
   "source": [
    "# load data, data augmentation, training, overfitting, transfer-learning etc.\n",
    "history=model.fit(x_train, y_train, epochs=40, verbose=1, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b45836",
   "metadata": {},
   "source": [
    "Calculating the **1 - Cohen's kappa** score of the trained model on the trained dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d705a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaulate_on():\n",
    "    predictions = np.zeros(len(y_eval), dtype=np.int8)\n",
    "\n",
    "    # inference loop\n",
    "    for e, (image, target) in enumerate(zip(x_eval, y_eval)):\n",
    "        image = np.expand_dims(np.array(image), axis=0)\n",
    "        output = model.predict(image)\n",
    "        predictions[e] = np.squeeze(output).argmax()\n",
    "\n",
    "    #Keras model score\n",
    "    score_keras = 1 - cohen_kappa_score(y_eval.numpy(), predictions)\n",
    "    print(y_eval.numpy(), predictions)\n",
    "    print(\"Score:\",score_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a89c8b",
   "metadata": {},
   "source": [
    "# 6. Saving and loading trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ec307",
   "metadata": {},
   "source": [
    "The trained model can be now saved by using HDF5-format that is the only accepted for submission. The name `test.h5` will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7170861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving model\n",
    "h5_filename = f'test_{datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}.h5'\n",
    "model.save_weights(h5_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3625f91",
   "metadata": {},
   "source": [
    "The trained model can be also loaded for further testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2ff1463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1a_ same\n",
      "block2a_ ((1, 1), (1, 1))\n",
      "block2a_ valid\n",
      "block2b_ same\n",
      "block3a_ ((2, 2), (2, 2))\n",
      "block3a_ valid\n",
      "block3b_ same\n",
      "block4a_ ((1, 1), (1, 1))\n",
      "block4a_ valid\n",
      "block4b_ same\n",
      "block4c_ same\n",
      "block5a_ same\n",
      "block5b_ same\n",
      "block5c_ same\n",
      "block6a_ ((2, 2), (2, 2))\n",
      "block6a_ valid\n",
      "block6b_ same\n",
      "block6c_ same\n",
      "block6d_ same\n",
      "block7a_ same\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNetLiteB0(classes=num_classes, weights=None, input_shape=input_shape, classifier_activation=None)\n",
    "model.load_weights(h5_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabf34d",
   "metadata": {},
   "source": [
    "The model will be now compiled and tested again. You should get the same score as before saving and loading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e583c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4607232729419548\n"
     ]
    }
   ],
   "source": [
    "#Model shall be compiled before testing.\n",
    "model.compile()\n",
    "\n",
    "#Creating empty predictions\n",
    "predictions = np.zeros(len(y_train), dtype=np.int8)\n",
    "\n",
    "# inference loop\n",
    "for e, (image, target) in enumerate(zip(x_train, y_train)):\n",
    "    image = np.expand_dims(np.array(image), axis=0)\n",
    "    output = model.predict(image)\n",
    "    predictions[e] = np.squeeze(output).argmax()\n",
    "\n",
    "#Keras model score\n",
    "score_keras = 1 - cohen_kappa_score(y_train.numpy(), predictions)\n",
    "print(\"Score:\",score_keras)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
